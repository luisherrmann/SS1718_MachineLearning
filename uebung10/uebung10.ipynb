{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Mustererkennung und Machine Learning</h1>\n",
    "\n",
    "<h3> Wintersemester 2017/2018, 10th Exercise Sheet</h3>\n",
    "<h4>Konstantin Jaehne, Luis Herrmann; Dozent: Ra√∫l Rojas</h4>\n",
    "\n",
    "<hr style='height:1px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we read in the digit dataset fron the respective files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 1813 data points of spam and 2788 points of non-spam.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.random as nprd\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "folderpath = '../'\n",
    "filename = 'spambase.data'\n",
    "data = pd.read_csv(folderpath+filename, header=None).as_matrix()\n",
    "\n",
    "spam = []\n",
    "nospam = []\n",
    "for sample in data:\n",
    "    if(sample[57] == 1):\n",
    "        spam.append(sample[:57])\n",
    "    elif(sample[57] == 0):\n",
    "        nospam.append(sample[:57])\n",
    "print('Data contains ' + str(len(spam)) + ' data points of spam and ' + str(len(nospam)) + ' points of non-spam.')\n",
    "rd_nospam = nprd.permutation(nospam)\n",
    "rd_spam = nprd.permutation(spam)\n",
    "train = (rd_nospam[:int(0.8 * len(nospam))], rd_spam[:int(0.8 * len(spam))])\n",
    "test = (rd_nospam[int(0.8 * len(nospam)):], rd_spam[int(0.8 * len(spam)):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we implement a AdaBoost Binary Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostBinary:\n",
    "    def __init__(self, data, classifiers, labels=None, itlimit=None):\n",
    "        \"\"\"\n",
    "        data:\n",
    "            Type: List/Tuple\n",
    "            A list/tuple (l1,l2), where l1 and l2 are lists containing vectors assigned to class 1 and 2, respectively.\n",
    "        classifiers:\n",
    "            Type: List/Tuple\n",
    "            A list/tuple k1,...,kn wheren ki is a binary classifer which for a matrix X[i,j] in which\n",
    "            X[i,:] is a feature vector, returns a vector for which v[i] is -1 or 1.\n",
    "        labels (optional):\n",
    "            Type: List/Tuple\n",
    "            A list/tuple (i1,i2) of two printable identifiers associated with each class\n",
    "        itlimit (optional):\n",
    "            Type: Integer\n",
    "            Maximum number of iterations, must be smaller than length of classifier list.\n",
    "        \"\"\"\n",
    "        if(not(labels is None)):\n",
    "            if(len(labels) == 2):\n",
    "                self.labels = np.array(labels)\n",
    "            else:\n",
    "                raise Exception('You must provide exactly two labels!')\n",
    "        else:\n",
    "            self.labels = np.array([0, 1], dtype=int)\n",
    "        if(not(data is None) and not(classifiers is None)):\n",
    "            self.train(data, classifiers, itlimit)\n",
    "            \n",
    "    def train(self, data, classifiers, itlimit=None):\n",
    "        if(not(itlimit is None) and itlimit > len(classifiers)):\n",
    "            raise Exception('itlimit has to be smaller than the number of classifiers!')\n",
    "        \"\"\"The k-classifiers have to return a -1, or 1\"\"\"\n",
    "        k = classifiers\n",
    "        Plen, Nlen = len(data[0]), len(data[1])\n",
    "        x = np.vstack(data)\n",
    "        x = np.hstack([x, np.repeat(1, x.shape[0])[:,np.newaxis]])\n",
    "        if(itlimit is None):\n",
    "            itlimit = len(classifiers)\n",
    "        #Create label vector:\n",
    "        y = np.repeat(1, x.shape[0])\n",
    "        y[:x.shape[0]] = 1\n",
    "        y[x.shape[0]:] = -1\n",
    "        #Create classifier assignment matrix with R[i,j] <- k_j(x_i) == y_i\n",
    "        R = np.zeros([x.shape[0], len(k)])\n",
    "        for i in range(len(k)):\n",
    "            R[:,i] = (k[i].classify(x) == y)\n",
    "        #Initialize weight vector\n",
    "        w = np.repeat(1, x.shape[0])\n",
    "        self.alpha = []\n",
    "        self.C = []\n",
    "        for i in range(itlimit):\n",
    "            #Select classifier that minimizes E_neq\n",
    "            indexmin= np.argmin([np.sum(w[np.argwhere(R[:,i] == 0)]) for i in range(len(k))])\n",
    "            E_eq = np.sum(w[np.argwhere(R[:,indexmin] == 1)], axis=0)\n",
    "            E_neq = np.sum(w[np.argwhere(R[:,indexmin] == 0)], axis=0)\n",
    "            #Recalculate weight vector\n",
    "            w = np.exp(-np.multiply(y, self.classify(x)))\n",
    "            self.alpha.append(1/2 * np.log(E_eq / E_neq))\n",
    "            self.C.append(k[indexmin])\n",
    "            #Remove selected classifier from list:\n",
    "            k.pop(indexmin)\n",
    "            np.delete(R, indexmin, axis=1)\n",
    "            sys.stdout.write(str(i+1) + ' iterations\\r')\n",
    "            \n",
    "    def classify(self, data):\n",
    "        s = np.zeros(data.shape[0])\n",
    "        for i in range(len(self.C)):\n",
    "            s += np.multiply(self.alpha[i], self.C[i].classify(data))\n",
    "        cls = np.zeros(s.shape, self.labels.dtype)\n",
    "        cls[s < 0.5] = self.labels[0]\n",
    "        cls[s >= 0.5] = self.labels[1]\n",
    "        return(np.array(s > 0.5, dtype=int))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import the LogisticClassifier-class from a past exercise sheet and apply some minor modifications to allow for outputs of labels -1 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticClassifier:\n",
    "    def __init__(self, data=None, labels=None, itlimit=1000, tol=1e-19, coords=None):\n",
    "        \"\"\"\n",
    "        data:\n",
    "            Type: List/Tuple\n",
    "            A list/tuple (l1,l2), where l1 and l2 are lists containing vectors assigned to class 1 and 2, respectively.\n",
    "        labels (optinal):\n",
    "            Type: List/Tuple\n",
    "            A list/tuple (i1,i2) of two printable identifiers associated with each class\n",
    "        itlimit (optional):\n",
    "            Type: Integer\n",
    "            Maximum number of iterations, before training is forcefully terminated.\n",
    "        tol (optional)\n",
    "            Type: Float\n",
    "            Minimum rate of improvement that is acceptable. If improvement drops below tolerance, stop descent.\n",
    "        \"\"\"\n",
    "        if(not(labels is None)):\n",
    "            if(len(labels) == 2):\n",
    "                self.labels = np.array(labels)\n",
    "            else:\n",
    "                raise Exception('You must provide exactly two labels!')\n",
    "        else:\n",
    "            self.labels = np.array([0, 1], dtype=int)\n",
    "        if(not(data is None)):\n",
    "            if(len(data) != 2):\n",
    "                raise Exception('Data does not follow specified format!')\n",
    "            if(len(data[0]) == 0 or len(data[1]) == 0):\n",
    "                raise Exception('Data lists must contain at least one sample!')\n",
    "            self.train(data, itlimit, tol, coords)\n",
    "    \n",
    "    def train(self, data, itlimit=1000, tol=1e-19, coords=None):\n",
    "        Plen, Nlen = len(data[0]), len(data[1])\n",
    "        self.x = np.vstack(data)\n",
    "        #Extract only desired coordinates\n",
    "        if(not(coords is None)):\n",
    "            self.coords = np.array(coords, dtype=int)\n",
    "            self.x = self.x[:,self.coords]\n",
    "        else:\n",
    "            self.coords = coords\n",
    "        #Calculate the variance for all features and set to 1 if zero. This will lead to no normalization.\n",
    "        self.variance = np.var(self.x, axis=0)[np.newaxis,:]\n",
    "        self.variance[np.where(self.variance == 0)] = 1\n",
    "        #Calculate the mean for all features\n",
    "        self.mean = np.mean(self.x, axis=0)[np.newaxis,:]\n",
    "        #Normalize data\n",
    "        self.x = self.normalize(self.x)\n",
    "        #Add column of 1s\n",
    "        self.x = np.hstack([np.ones(Plen+Nlen)[:,np.newaxis], self.x])\n",
    "        self.y = np.append(np.repeat(1.0, Plen), np.repeat(-1.0, Nlen))\n",
    "        #self.y = np.append(np.repeat(1.0, Plen), np.repeat(0.0, Nlen))\n",
    "        #self.w = self.x[rd.randint(0, self.x.shape[1]-1)]\n",
    "        self.w = nprd.random(self.x.shape[1])\n",
    "        self.offset = 0\n",
    "        stepsize = 1e+1\n",
    "        t = 0\n",
    "        while(t < itlimit):\n",
    "            weights = self.y * self.sigmoid(np.multiply(-self.y, np.dot(self.x, self.w)))\n",
    "            #weights = self.y - self.sigmoid(np.dot(self.x, self.w))\n",
    "            gradient =  np.sum(np.multiply(self.x, weights[:,np.newaxis]), axis=0)\n",
    "            #Back-tracking: The measure for improvements is the avg of probabilitis rather than the likelihood function\n",
    "            while(self.avgprob(self.w + stepsize * gradient) < self.avgprob(self.w + (stepsize / 2) * gradient)):\n",
    "                stepsize /= 2\n",
    "            #If the improvement drops below a certain tolerance, quit ascent\n",
    "            if(self.avgprob(self.w + stepsize * gradient) - self.avgprob(self.w) < tol):\n",
    "                break\n",
    "            self.w += stepsize * gradient\n",
    "            t += 1\n",
    "            sys.stdout.write(str(t) + ' iterations, stepsize %.3e' % stepsize + '\\r')\n",
    "        sys.stdout.write('\\nTraining complete!\\n')\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        \"\"\"Normalizes the data to allow for faster convergence of the gradient ascent\"\"\"\n",
    "        return((data - self.mean) / self.variance)\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"Classifies a vector\"\"\"\n",
    "        prob = self.prob(data)\n",
    "        cls = np.zeros(prob.shape, self.labels.dtype)\n",
    "        cls[prob > 0.5] = self.labels[0]\n",
    "        cls[prob <= 0.5] = self.labels[1]\n",
    "        return(cls)\n",
    "    \n",
    "    def prob(self, data):\n",
    "        \"\"\"Returns probabilities for a vector to be in the positive class\"\"\"\n",
    "        if(not(self.coords is None)):\n",
    "            data = data[:,self.coords]\n",
    "        return(self.sigmoid(np.dot(np.hstack([self.normalize(data), np.ones([data.shape[0],1])]), self.w) - self.offset))\n",
    "    \n",
    "    def avgprob(self, w):\n",
    "        \"\"\"Given a perceptron w, returns the average probability for the data to be in the positive class\"\"\"\n",
    "        return(np.sum(self.sigmoid(np.multiply(self.y, np.dot(self.x, w))))/self.x.shape[0])\n",
    "    \n",
    "    def likelihood(self, w):\n",
    "        \"\"\"Given a perceptron w, returns the likelihood for the data to be in the positive class\"\"\"\n",
    "        return(np.prod(self.sigmoid(np.multiply(self.y, np.dot(self.x, w)))))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"The sigmoidal functinon as defined by the numerically more stable tanh\"\"\"\n",
    "        return(1/2 * (1 + np.tanh(1/2 * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def runtest(C, data, silent=False):\n",
    "    n = len(C.labels)\n",
    "    confusionMat = np.zeros([n, n], dtype=int)\n",
    "    for i in range(n):\n",
    "        result = C.classify(data[i])\n",
    "        for j in range(len(result)):\n",
    "            confusionMat[i,C.labels == result[j]] += 1\n",
    "    err = 1-sum(np.diag(confusionMat))/np.sum(confusionMat)\n",
    "    if(not(silent)):\n",
    "        print('-The confusion matrix is given by:')\n",
    "        html = pd.DataFrame(confusionMat,index=C.labels, columns=C.labels).to_html()\n",
    "        display(HTML(html))\n",
    "        print('-The error rate is: ' + str(err) + '\\n')\n",
    "    return(confusionMat, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to create a list of weak classifiers, for which the error rate is smaller than 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0:\n",
      "\n",
      "5 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "6 iterations, stepsize 9.766e-03\n",
      "Training complete!\n",
      "3 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "3 iterations, stepsize 1.250e+00\n",
      "Training complete!\n",
      "10 iterations, stepsize 1.192e-06\n",
      "Training complete!\n",
      "3 iterations, stepsize 3.906e-02\n",
      "Training complete!\n",
      "3 iterations, stepsize 2.500e+00\n",
      "Training complete!\n",
      "4 iterations, stepsize 3.906e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "6 iterations, stepsize 1.526e-04\n",
      "Training complete!\n",
      "5 iterations, stepsize 1.526e-04\n",
      "Training complete!\n",
      "5 iterations, stepsize 2.441e-03\n",
      "Training complete!\n",
      "6 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 6.250e-01\n",
      "Training complete!\n",
      "3 iterations, stepsize 2.500e+00\n",
      "Training complete!\n",
      "5 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "4 iterations, stepsize 7.812e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "12 iterations, stepsize 3.052e-04\n",
      "Training complete!\n",
      "5 iterations, stepsize 3.906e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 6.250e-01\n",
      "Training complete!\n",
      "2 iterations, stepsize 1.000e+01\n",
      "Training complete!\n",
      "4 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "3 iterations, stepsize 2.500e+00\n",
      "Training complete!\n",
      "5 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "7 iterations, stepsize 4.768e-06\n",
      "Training complete!\n",
      "5 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "3 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "3 iterations, stepsize 2.500e+00\n",
      "Training complete!\n",
      "3 iterations, stepsize 4.883e-03\n",
      "Training complete!\n",
      "4 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "4 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "2 iterations, stepsize 1.000e+01\n",
      "Training complete!\n",
      "3 iterations, stepsize 5.000e+00\n",
      "Training complete!\n",
      "6 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "8 iterations, stepsize 9.537e-06\n",
      "Training complete!\n",
      "2 iterations, stepsize 1.000e+01\n",
      "Training complete!\n",
      "6 iterations, stepsize 7.812e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 1.250e+00\n",
      "Training complete!\n",
      "3 iterations, stepsize 9.766e-03\n",
      "Training complete!\n",
      "4 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "3 iterations, stepsize 5.000e+00\n",
      "Training complete!\n",
      "5 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "3 iterations, stepsize 4.883e-03\n",
      "Training complete!\n",
      "11 iterations, stepsize 6.104e-04\n",
      "Training complete!\n",
      "3 iterations, stepsize 5.000e+00\n",
      "Training complete!\n",
      "4 iterations, stepsize 7.812e-02\n",
      "Training complete!\n",
      "5 iterations, stepsize 3.052e-04\n",
      "Training complete!\n",
      "4 iterations, stepsize 3.906e-02\n",
      "Training complete!\n",
      "6 iterations, stepsize 4.883e-03\n",
      "Training complete!\n",
      "3 iterations, stepsize 9.766e-03\n",
      "Training complete!\n",
      "5 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "5 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 7.812e-02\n",
      "Training complete!\n",
      "4 iterations, stepsize 6.250e-01\n",
      "Training complete!\n",
      "5 iterations, stepsize 1.110e-15\n",
      "Training complete!\n",
      "5 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "3 iterations, stepsize 6.250e-01\n",
      "Training complete!\n",
      "6 iterations, stepsize 7.629e-05\n",
      "Training complete!\n",
      "4 iterations, stepsize 6.250e-01\n",
      "Training complete!\n",
      "4 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "4 iterations, stepsize 1.250e+00\n",
      "Training complete!\n",
      "3 iterations, stepsize 1.250e+00\n",
      "Training complete!\n",
      "5 iterations, stepsize 4.883e-03\n",
      "Training complete!\n",
      "3 iterations, stepsize 7.812e-02\n",
      "Training complete!\n",
      "5 iterations, stepsize 6.250e-01\n",
      "Training complete!\n",
      "6 iterations, stepsize 4.883e-03\n",
      "Training complete!\n",
      "2 iterations, stepsize 1.000e+01\n",
      "Training complete!\n",
      "4 iterations, stepsize 9.766e-03\n",
      "Training complete!\n",
      "9 iterations, stepsize 1.526e-04\n",
      "Training complete!\n",
      "3 iterations, stepsize 1.221e-03\n",
      "Training complete!\n",
      "4 iterations, stepsize 3.052e-04\n",
      "Training complete!\n",
      "5 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "7 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "5 iterations, stepsize 7.812e-02\n",
      "Training complete!\n",
      "2 iterations, stepsize 1.953e-02\n",
      "Training complete!\n",
      "-The error rate is: 0.358306188925\n",
      "Classifier 1:\n",
      "\n",
      "3 iterations, stepsize 1.000e+01\n",
      "Training complete!\n",
      "-The error rate is: 0.268186753529\n",
      "Classifier 2:\n",
      "\n",
      "3 iterations, stepsize 2.500e+00\n",
      "Training complete!\n",
      "-The error rate is: 0.403908794788\n",
      "Classifier 3:\n",
      "\n",
      "3 iterations, stepsize 1.000e+01\n",
      "Training complete!\n",
      "-The error rate is: 0.234527687296\n",
      "Classifier 4:\n",
      "\n",
      "9 iterations, stepsize 4.883e-03\n",
      "Training complete!\n",
      "-The error rate is: 0.230184581976\n",
      "Classifier 5:\n",
      "\n",
      "2 iterations, stepsize 3.052e-04\n",
      "Training complete!\n",
      "-The error rate is: 0.401737242128\n",
      "Classifier 6:\n",
      "\n",
      "1 iterations, stepsize 4.883e-03\n",
      "Training complete!\n",
      "2 iterations, stepsize 7.629e-05\n",
      "Training complete!\n",
      "\n",
      "Training complete!\n",
      "2 iterations, stepsize 1.250e+00\n",
      "Training complete!\n",
      "3 iterations, stepsize 6.250e-01\n",
      "Training complete!\n",
      "-The error rate is: 0.394136807818\n",
      "Classifier 7:\n",
      "\n",
      "\n",
      "Training complete!\n",
      "-The error rate is: 0.457111834962\n",
      "Classifier 8:\n",
      "\n",
      "3 iterations, stepsize 3.125e-01\n",
      "Training complete!\n",
      "2 iterations, stepsize 2.500e+00\n",
      "Training complete!\n",
      "-The error rate is: 0.461454940282\n",
      "Classifier 9:\n",
      "\n",
      "\n",
      "Training complete!\n",
      "-The error rate is: 0.42453854506\n",
      "Classifier 10:\n",
      "\n",
      "9 iterations, stepsize 1.562e-01\n",
      "Training complete!\n",
      "-The error rate is: 0.312703583062\n"
     ]
    }
   ],
   "source": [
    "C = []\n",
    "ranges = [list(range(5*i, 5*(i+1))) for i in range(10)] + [list(range(50,57))]\n",
    "for i in range(len(ranges)):\n",
    "    print('Classifier ' + str(i) + ':\\n')\n",
    "    while(True):\n",
    "        Ci = LogisticClassifier(train, labels=(-1,1), itlimit=1000, coords= ranges[i])\n",
    "        confusionMat, err  = runtest(Ci, test, silent=True);\n",
    "        if(err < 0.5):\n",
    "            print('-The error rate is: ' + str(err))\n",
    "            break\n",
    "    C.append(Ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed these to our implementation of Adaboost to obtain a new classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations\r",
      "2 iterations\r",
      "3 iterations\r",
      "4 iterations\r",
      "5 iterations\r",
      "6 iterations\r",
      "7 iterations\r",
      "8 iterations\r",
      "9 iterations\r",
      "10 iterations\r",
      "11 iterations\r",
      "-The confusion matrix is given by:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>382</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-The error rate is: 0.260586319218\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[382, 176],\n",
       "        [ 64, 299]]), 0.26058631921824105)"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBoost = AdaBoostBinary(train, C.copy())\n",
    "runtest(CBoost, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
